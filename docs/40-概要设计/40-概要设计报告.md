# 系统概要设计

## 0. 文档定位与引用关系

本文档阐述系统的总体架构与关键机制设计，涵盖以下核心内容：

中心机采用"单定时器顺序流水线"架构，轮询、存储、检测、入图四个步骤串行执行，保证数据流转的因果顺序。前端通过后端 API 访问数据，实现图谱查询与事件检索。溯源分析采用"异步任务"模型：耗时的图算法与分析操作以任务形式异步执行，结果写回 Neo4j 边属性供前端展示。OpenSearch、Neo4j 与 Analysis 三大模块协同工作，支撑从数据采集到溯源分析的全流程。

具体实现细节引用下列文档：

字段口径遵循 `../80-规范/81-ECS字段规范.md`；图谱模型见 `../80-规范/84-Neo4j实体图谱规范.md`；环境配置参考 `../80-规范/89-环境变量与配置规范.md`。客户机与中心机的接口约定定义于 `../80-规范/87-客户机与中心机接口.md`。OpenSearch 存储与索引治理详见 `../50-详细设计/中心机/62-OpenSearch存储与索引治理.md`，Neo4j 入图与图查询逻辑见 `../50-详细设计/中心机/64-Neo4j入图与图查询.md`，Analysis 模块的任务模型则由 `../50-详细设计/分析/70-任务模型与状态机.md` 阐述。

## 1. 总体架构

### 1.1 术语与缩略语

为保证表述一致，本节定义核心术语：

| 术语 | 英文 | 定义 |
|------|------|------|
| **客户机** | Client | 部署在被监控主机上的数据采集与转换节点，采集 Falco、Filebeat、Suricata 三类数据源，转换为 ECS 格式并缓冲在本地队列 |
| **中心机** | Center Server | 汇聚数据、执行检测与分析的核心服务节点，轮询拉取、入库、检测融合、入图、对外 API 与异步溯源任务 |
| **Telemetry** | Telemetry | 遥测数据，指未经检测规则触发的原始观测事件，`event.kind="event"` |
| **Raw Finding** | Raw Finding | 单一检测规则（如 Sigma 规则、Security Analytics 检测器）产生的初步告警，`event.kind="alert"` 且 `dataset!="canonical"` |
| **Canonical Finding** | Canonical Finding | 融合去重后的标准化告警，`event.kind="alert"` 且 `dataset="finding.canonical"`，用于图谱入图与溯源分析主输入 |
| **溯源任务** | Trace Task | 以目标节点为起点、时间窗为边界的溯源分析任务，异步执行并在完成后将结果写回 Neo4j 边属性 |
| **TTP** | Tactics, Techniques, and Procedures | 战术、技术与过程，指 MITRE ATT&CK 框架中对攻击行为的分类与描述 |
| **ECS** | Elastic Common Schema | Elastic 公司定义的通用事件字段规范，用于统一不同数据源的字段命名 |
| **实体图谱** | Entity Graph | 以 Host、User、Process、File、IP、Domain 等实体为节点、以事件为边构成的属性图，存储于 Neo4j |
| **关键路径** | Critical Path | 溯源任务计算出的攻击传播路径，边属性 `analysis.is_path_edge=true` 标记 |
| **顺序流水线** | Sequential Pipeline | 中心机使用单个定时器串行执行轮询、存储、检测、入图四个步骤的架构模式 |

### 1.2 系统组成

整个系统由客户机与中心机协同构成。客户机从 Falco、Filebeat、Suricata 等传感器获取原始数据，转换为统一的 ECS 格式后暂存于本地队列，通过拉取接口向中心机提供数据。中心机汇聚所有客户机的数据，依次完成入库存储、规则检测、告警融合、图谱构建等处理流程，同时对外提供 API 接口、执行异步溯源任务并生成分析报告。

### 1.3 系统架构图（逻辑视图）

```mermaid
flowchart TB
    subgraph Clients["客户机集群 (≥5 节点)"]
        direction LR
        C1["client-01<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C2["client-02<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C3["client-03<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C4["..."]
        C5["client-N<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
    end

    subgraph Backend["中心机后端 (FastAPI :8001)"]
        Scheduler["定时流水线调度器<br/>单定时器 (5s 周期)"]
        OS_Mod["OpenSearch 模块<br/>入库/检测/融合"]
        Neo4j_Mod["Neo4j 模块<br/>入图/图查询"]
        Analysis_Mod["Analysis 模块<br/>异步溯源任务"]
        API["对外 API<br/>/api/v1/*"]
    end

    subgraph Storage["存储层"]
        OS["OpenSearch :9200<br/>├─ Telemetry 索引<br/>├─ Findings 索引<br/>└─ Task 索引"]
        Neo4j["Neo4j :7687<br/>├─ Entity Graph<br/>├─ 图查询/最短路<br/>└─ 边属性 (溯源结果)"]
    end

    subgraph Frontend["中心机前端 (Next.js :3000)"]
        UI["图谱可视化 UI<br/>├─ 事件/告警查询<br/>├─ 图谱展示<br/>├─ 溯源任务<br/>└─ 报告导出"]
    end

    Clients ==>|"定时拉取<br/>GET /falco,/filebeat,/suricata"| Scheduler
    Scheduler --> OS_Mod
    Scheduler --> Neo4j_Mod
    Scheduler --> Analysis_Mod

    OS_Mod -.->|"写入/读取"| OS
    Neo4j_Mod -.->|"写入/查询"| Neo4j
    Analysis_Mod -.->|"任务状态"| OS
    Analysis_Mod -.->|"结果写回"| Neo4j

    API <--> Scheduler
    API <--> Neo4j_Mod
    API <--> Analysis_Mod

    UI <-->|"HTTP 请求"| API
    UI <-->|"WebSocket"| API

    classDef clientStyle fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef backendStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c
    classDef storageStyle fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#4a148c
    classDef frontendStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20

    class C1,C2,C3,C4,C5 clientStyle
    class Scheduler,OS_Mod,Neo4j_Mod,Analysis_Mod,API backendStyle
    class OS,Neo4j storageStyle
    class UI frontendStyle
```

## 2. 核心数据分层（中心机视角）

### 2.1 数据分层架构

系统采用以下数据层次（从底到顶）：

```mermaid
flowchart LR
    subgraph L1["Layer 1: Telemetry 事实层"]
        T1["ECS 事件<br/>event.kind 为 event"]
        T2["OpenSearch<br/>ecs-events-*"]
    end

    subgraph L2["Layer 2: Raw Findings 原始告警层"]
        R1["传感器告警<br/>Security Analytics<br/>event.kind 为 alert<br/>dataset 非 canonical"]
        R2["OpenSearch<br/>raw-findings-*"]
    end

    subgraph L3["Layer 3: Canonical Findings 规范告警层"]
        C1["融合去重<br/>event.kind 为 alert<br/>dataset 为 finding.canonical"]
        C2["OpenSearch<br/>canonical-findings-*"]
    end

    subgraph L4["Layer 4: Entity Graph 图谱层"]
        G1["节点/边<br/>Host/User/Process<br/>File/IP/Domain"]
        G2["Neo4j<br/>Entity Graph"]
    end

    subgraph L5["Layer 5: Trace Task 溯源任务层"]
        A1["任务状态 + 结果<br/>task_id<br/>analysis.* 字段"]
        A2["OpenSearch<br/>analysis-tasks-*"]
        A3["Neo4j<br/>边属性写回"]
    end

    T1 --> T2
    R1 --> R2
    C1 --> C2
    G1 --> G2
    A1 -.->|"状态"| A2
    A1 -.->|"结果"| A3

    T2 -.->|"检测"| R1
    R2 -.->|"融合"| C1
    T2 & C1 -.->|"ECS→Graph"| G1
    G2 -.->|"读取"| A1
    C2 -.->|"输入"| A1

    classDef layerStyle fill:#f5f5f5,stroke:#424242,stroke-width:2px,color:#212121
    classDef storageStyle fill:#e1bee7,stroke:#6a1b9a,stroke-width:2px,color:#4a148c
    classDef dataStyle fill:#bbdefb,stroke:#1565c0,stroke-width:2px,color:#0d47a1

    class L1,L2,L3,L4,L5 layerStyle
    class T2,R2,C2,A2 storageStyle
    class G2,A3 storageStyle
    class T1,R1,C1,G1,A1 dataStyle
```

### 2.2 数据层次说明

系统中数据自底向上逐层演进。底层的事实层（Telemetry）存储传感器产出的原始观测事件，以 `event.kind="event"` 标识，是所有检测与分析的源头。原始告警层（Raw Findings）汇聚单一检测规则触发的初步告警，`event.kind="alert"` 且 `dataset` 不为 canonical，保留各传感器的原始特征。规范告警层（Canonical Findings）对 Raw Findings 进行融合去重，生成标准化的告警对象，作为图谱入图与溯源分析的主要输入。图谱层（Entity Graph）将 Telemetry 与 Canonical Findings 转换为以 Host、User、Process、File 等实体为节点、事件为边的属性图，支撑复杂的关系查询与路径分析。最顶层的溯源任务层（Trace Task）记录异步分析任务的状态与结果：在 OpenSearch 中保存任务文档，并将分析结论写回 Neo4j 边属性，形成"被标注的图"。

| 层次 | 名称 | 判定条件 | 存储位置 | 用途 |
|------|------|----------|----------|------|
| **Layer 1** | Telemetry（事实层） | `event.kind="event"` | OpenSearch `ecs-events-*` | 原始事件检索、检测输入 |
| **Layer 2** | Raw Findings（原始告警层） | `event.kind="alert"` 且 `dataset!="canonical"` | OpenSearch `raw-findings-*` | 原始告警审计、融合输入 |
| **Layer 3** | Canonical Findings（规范告警层） | `event.kind="alert"` 且 `dataset="canonical"` | OpenSearch `canonical-findings-*` | 图谱与溯源主输入 |
| **Layer 4** | Entity Graph（图谱层） | Telemetry + Canonical 转换 | Neo4j 节点/边 | 图查询、路径分析 |
| **Layer 5** | Trace Task（溯源任务层） | 异步任务创建 | OpenSearch `analysis-tasks-*` + Neo4j 边属性 | 任务状态、溯源结果 |

> 说明：攻击链作为"展示结构"由图谱与任务结果共同承载：前端展示时以图谱边序列和边属性为主，不依赖单独的"链路索引"。

## 3. 中心机定时流水线（单定时器顺序流水线）

### 3.1 调度原则

中心机流水线的调度遵循简单而严格的规则。系统使用单一定时器触发流水线，周期由环境变量 `CENTER_POLL_INTERVAL_SECONDS` 控制，默认值为 5 秒。为确保数据一致性，同一时间只允许一个流水线实例运行：当上一次流水线尚未完成时，新的定时触发会被自动跳过，避免并发写入导致的事件顺序错乱与重复处理。

### 3.1.1 设计决策：为什么采用单定时器顺序流水线架构

采用单定时器串行执行架构，而非并发或事件驱动模式，基于以下核心考量：

数据一致性是首要目标。串行执行确保 Telemetry → Raw → Canonical → Graph 的严格因果顺序，避免并发场景下的事件乱序与状态不一致。资源可控性同样重要：单线程模型使 CPU、内存与网络带宽的消耗可预测，避免高并发下的资源竞争与突发峰值。从运维角度看，串行流水线的调用链清晰透明，便于通过日志快速定位问题根因。此外，这种架构避免引入分布式锁、消息队列去重等复杂机制，降低了实现复杂度与出错概率。

当然，这一选择也带来相应的代价：吞吐量上限受限于单线程处理速度，预计可支撑约 10 个客户机的规模；同时单步骤故障会阻塞后续步骤，需要通过快速失败机制与错误隔离策略来缓解影响。

### 3.3 流水线输入与输出

流水线的输入来自所有已注册客户机的新增数据，通过轮询拉取接口获取，格式为标准的 ECS 文档。输出流向两个存储后端：OpenSearch 接收 Telemetry、Raw Findings、Canonical Findings 与 Tasks 四类数据；Neo4j 存储实体图谱节点与边，包括溯源任务分析写回的边属性标注。

### 3.4 四步顺序与职责（每次 tick 内严格顺序执行）

#### Step 1：从客户机拉取数据

对每个已注册客户机：

1) 中心机读取注册表条目（含 `listen_url`、`client_token_hash` 等元数据）
2) 调用客户机拉取接口（见 `../80-规范/87-客户机与中心机接口.md`）
3) 客户机从 RabbitMQ 队列中取出消息并返回 ECS 文档列表：
   - 队列为空则返回空数组
   - 拉取行为会消费队列中的消息（队列语义保证增量）

#### Step 2：写入 OpenSearch（字段处理）

中心机必须对每条 ECS 文档做字段处理（细节见 `../50-详细设计/中心机/62-OpenSearch存储与索引治理.md` 和 `../80-规范/81-ECS字段规范.md`）：

- 三时间字段补齐与覆盖规则
- `event.kind` 与 `event.dataset` 校验
- `event.id` 补齐与幂等去重
- 按 `event.kind/event.dataset` 路由写入对应索引

#### Step 3：Store-first 检测 + Raw→Canonical 融合

中心机在每个 tick 内执行以下操作：

1) 触发 Store-first 检测：由 OpenSearch Security Analytics 对 Telemetry 索引执行扫描
2) 读取检测产生的 Findings，并转换为 ECS Finding 文档，写入 `raw-findings-*`
3) 对"指定时间窗内"的 Raw Findings 做融合去重，生成 Canonical Findings，写入 `canonical-findings-*`

融合去重的指纹规则、provider 合并规则与幂等规则由 `../50-详细设计/中心机/62-OpenSearch存储与索引治理.md` 定义。

> 实现口径（与当前代码对齐）：Step 3 在每个 tick 内固定执行（不提供开关），用于持续产出 Raw→Canonical 的规范告警层。

#### Step 4：触发 ECS→Graph 写入 Neo4j

中心机在每个 tick 内执行入图操作：

1) 以 Canonical Findings 为主，补充必要的 Telemetry（用于补边与证据回溯）
2) 对每条输入文档执行 ECS→Graph 转换
3) 将节点/边写入 Neo4j，并满足 `../80-规范/84-Neo4j实体图谱规范.md` 的唯一键与边属性规范
4) 边必须写入 `ts_float`（数值时间戳），支撑时间窗查询与图算法投影

> 实现口径（与当前代码对齐）：Step 4 在每个 tick 内固定执行；入图以 `event.ingested`（中心机入库时间）为窗口边界，确保本 tick 生成的数据不会遗漏。

### 3.5 流水线时序图

```mermaid
sequenceDiagram
    autonumber
    participant Timer as 定时器<br/>(5s)
    participant Poller as 轮询服务
    participant Clients as 客户机集群
    participant OS as OpenSearch
    participant SA as Security<br/>Analytics
    participant Neo4j as Neo4j

    Note over Timer,Neo4j: 单次 tick 流水线（严格顺序执行）

    Timer->>Poller: 触发 tick
    activate Poller

    rect rgb(240, 248, 255)
        Note right of Poller: Step 1: 拉取数据
        Poller->>OS: 读取 client-registry
        OS-->>Poller: 返回客户机列表

        loop 每个已注册客户机
            Poller->>Clients: GET /falco,/filebeat,/suricata
            Clients-->>Poller: ECS 事件列表
        end
    end

    rect rgb(255, 243, 224)
        Note right of Poller: Step 2: 写入 OpenSearch
        Poller->>OS: store_events(批量事件)
        Note right of OS: 字段处理<br/>- 三时间字段<br/>- event.id 去重<br/>- 索引路由
        OS-->>Poller: 写入完成
    end

    rect rgb(243, 255, 240)
        Note right of Poller: Step 3: 检测与融合
        Poller->>SA: 触发 Store-first 检测
        SA->>OS: 扫描 ecs-events-*
        SA-->>Poller: 生成 Raw Findings

        Poller->>OS: 融合去重 (Raw→Canonical)
        Note right of OS: 指纹规则<br/>- threat.technique.id<br/>- host.id<br/>- entity_id<br/>- time_bucket
        OS-->>Poller: 写入 Canonical Findings
    end

    rect rgb(255, 240, 245)
        Note right of Poller: Step 4: 入图 Neo4j
        Poller->>OS: 查询本 tick 新数据
        OS-->>Poller: Canonical + Telemetry

        Poller->>Neo4j: 批量写入节点/边
        Note right of Neo4j: ECS→Graph 转换<br/>- MERGE 节点<br/>- MERGE 边<br/>- ts_float 属性
        Neo4j-->>Poller: 入图完成
    end

    deactivate Poller
    Note over Timer: 等待下次触发 (5s)
```

## 4. 前端可视化查询链路

### 4.1 查询边界

前端与后端的数据交互遵循清晰的职责划分。OpenSearch 负责 Telemetry 与 Findings 的检索，Neo4j 承担图谱节点、边的查询与路径计算。前端禁止直连数据库，所有数据访问必须经过后端 API：这一设计既保护存储层的安全，也便于统一权限控制与审计。

### 4.2 图可视化最小闭环

图谱可视化的交互形成完整闭环。用户在前端发起图查询请求，后端调用 Neo4j 模块执行时间窗边查询、告警边查询或最短路计算等操作，返回 nodes 与 edges 数据。前端接收图数据后进行渲染，用户可点击节点查看属性，或通过边的证据引用追溯到原始事件。这一闭环通过 `/api/v1/graph/query` 等 API 实现，模块内部的查询规则详见 `../50-详细设计/中心机/64-Neo4j入图与图查询.md`。

## 5. 异步溯源任务（create_task）

### 5.1 为什么必须异步

溯源任务采用异步模式，根因在于其计算复杂度与耗时不确定性。一次完整的节点溯源需要执行子图扩展、多跳邻居查询，在此基础上运行图算法（如最短路、风险权重路径计算），最终还要生成包含 TTP 解释与文本说明的可读性报告。这些操作的耗时从数秒到数十秒不等，若采用同步调用会导致前端长时间阻塞，用户体验极差。异步任务模型允许后端立即返回 task_id，前端通过轮询获取进度，既避免超时风险，也为实时反馈任务状态提供了可能。

### 5.2 任务模型（状态机）

每个溯源任务在 OpenSearch `analysis-tasks-*` 里保存一条任务文档，任务状态只能取以下值：

```mermaid
stateDiagram-v2
    [*] --> queued: 创建任务

    queued --> running: 任务执行器取走

    running --> succeeded: 执行成功
    running --> failed: 执行失败

    succeeded --> [*]
    failed --> [*]

    note right of queued
        task.progress = 0
        task.created_at
    end note

    note right of running
        task.progress = 5-95
        任务执行中
    end note

    note right of succeeded
        task.progress = 100
        task.finished_at
        结果写回 Neo4j
    end note

    note right of failed
        task.progress < 100
        task.finished_at
        task.error
    end note
```

**状态说明**：

- `queued`：已创建，等待执行
- `running`：执行中
- `succeeded`：已完成
- `failed`：失败（含错误信息）

**状态转移约束**：
- 只允许 `queued → running → succeeded/failed` 单向转移
- 禁止回退与跳转（如 `running → queued`、`queued → succeeded`）

### 5.3 创建、执行与结果写回

溯源任务的完整生命周期包含五个阶段。用户在前端图谱上选择目标节点并发起创建请求，后端立即生成并返回 task_id，实现"快速响应"。Analysis 模块异步接管任务执行：首先从 Neo4j 读取相关子图数据，然后运行图算法提取关键路径，计算风险评分，汇总涉及的 Technique/Tactic，最后生成可读性解释文本。分析结果被写回 Neo4j 的边属性，形成"被标注的图"，将分析结论与图谱数据紧密耦合。前端通过轮询 `GET /api/v1/analysis/tasks/{task_id}` 接口追踪任务进度，待任务完成后再次请求图查询接口读取边属性，即可在界面上展示溯源结果。

任务创建、状态查询与结果拉取分别通过 `POST /api/v1/analysis/tasks`、`GET /api/v1/analysis/tasks/{task_id}` 和 `POST /api/v1/graph/query` 等 API 实现。写回字段的命名规则、覆盖策略与数据类型由 `../50-详细设计/分析/70-任务模型与状态机.md` 和 `../50-详细设计/中心机/64-Neo4j入图与图查询.md` 共同定义。

### 5.4 KillChain 分析（内部实现）

溯源任务的核心算法是 KillChain，负责从离散的攻击事件中重建完整的攻击路径。KillChain 基于 MITRE ATT&CK 框架的战术分类体系，通过有限状态自动机识别攻击所处的阶段（如 Initial Access、Execution、Privilege Escalation、Lateral Movement、C2、Impact），然后利用大语言模型从候选路径中智能选择最合理的段间连接，最终生成覆盖全链路的可读性解释。

这一算法的能力体现在五个方面：战术分段，自动将杂乱的攻击事件按 ATT&CK 战术归类，形成有意义的阶段划分；路径重建，识别各战术阶段之间的跳转关系；智能选择，借助 LLM 的语义理解能力从多条候选路径中筛选出最符合攻击者意图的路径；可解释性，生成 10-20 句结构完整（包含主谓宾）的中文解释，让非专家用户也能理解攻击过程；置信度评估，输出 0.0-1.0 之间的可信度评分，帮助用户判断结论的可靠程度。

KillChain 的输出包含四个核心数据结构：`segments[]` 数组记录每个战术分段的状态、时间范围、锚点与异常边摘要；`selected_paths[]` 数组存储段间连接路径；`explanation` 字段保存 LLM 生成的完整解释文本；`confidence` 字段给出可信度评分。持久化采用双重写入机制：Neo4j 边属性存储 `custom.killchain.uuid`（符合 ECS 规范），任务文档则保存 `task.result.killchain_uuid` 和 `task.result.killchain`。详细设计参见 `50-详细设计/分析/69-KillChain概览设计.md` 和 `74-KillChain结果展示规范.md`。

## 6. 安全与审计边界（课程演示）

系统部署于靶场内网环境中，所有中心机服务与数据库端口仅对靶场内部网络开放，形成天然的隔离边界。CTI（威胁情报）数据采用离线的 ATT&CK Enterprise 数据包，运行时不依赖外网连接，确保演示环境的稳定性与可复现性。审计方面，任何分析结论都必须能够回溯到具体的 `event.id`，并明确标注数据源类型和时间窗范围，形成完整的证据链，确保每个结论都有据可查。
