# 攻击溯源分析系统概要设计

> 本文档由以下文档合并而成：

1. [40-概要设计报告.md](40-概要设计报告.md)
2. [41-数据流与时序.md](41-数据流与时序.md)
3. [42-部署拓扑与网络规划.md](42-部署拓扑与网络规划.md)
4. [43-非功能设计.md](43-非功能设计.md)

---



# 一、概要设计报告


本文档阐述系统总体架构与核心机制，涵盖以下设计要点：

- 中心机单定时器顺序流水线架构
- 前端可视化查询链路
- 异步溯源任务模型
- OpenSearch、Neo4j、Analysis 三大模块的边界划分与协作机制

技术细节已由以下文档专门说明，本文档不再重复：

- **字段口径**：`../80-规范/81-ECS字段规范.md`
- **图谱规范**：`../80-规范/84-Neo4j实体图谱规范.md`
- **环境配置**：`../80-规范/89-环境变量与配置规范.md`
- **接口定义**：`../80-规范/87-客户机与中心机接口.md`
- **OpenSearch 存储**：`../50-详细设计/中心机/62-OpenSearch存储与索引治理.md`
- **Neo4j 图查询**：`../50-详细设计/中心机/64-Neo4j入图与图查询.md`
- **Analysis 任务模型**：`../50-详细设计/分析/70-任务模型与状态机.md`

## 1. 总体架构
### 1.1 术语与缩略语
本节统一核心术语的定义与用法：

| 术语 | 英文 | 定义 |
|------|------|------|
| **客户机（Client）** | Client | 部署在被监控主机上的数据采集与转换节点。采集 Falco、Filebeat、Suricata 三类数据源，转换为 ECS 格式并缓冲在本地队列 |
| **中心机（Center）** | Center Server | 汇聚数据、执行检测与分析的核心服务节点。处理轮询拉取、入库、检测融合、入图、对外 API 与异步溯源任务 |
| **Telemetry** | Telemetry | 遥测数据，未经检测规则触发的原始观测事件，`event.kind="event"` |
| **Raw Finding** | Raw Finding | 单一检测规则（如 Sigma 规则、Security Analytics 检测器）产生的初步告警，`event.kind="alert"` 且 `dataset!="canonical"` |
| **Canonical Finding** | Canonical Finding | 融合去重后的标准化告警，`event.kind="alert"` 且 `dataset="finding.canonical"`，用于图谱入图与溯源分析主输入 |
| **溯源任务（Trace Task）** | Trace Task | 以目标节点为起点、时间窗为边界的溯源分析任务。异步执行，完成后将结果写回 Neo4j 边属性 |
| **TTP** | Tactics, Techniques, and Procedures | 战术、技术与过程，MITRE ATT&CK 框架中对攻击行为的分类与描述 |
| **ECS** | Elastic Common Schema | Elastic 公司定义的通用事件字段规范，统一不同数据源的字段命名 |
| **实体图谱** | Entity Graph | 以 Host、User、Process、File、IP、Domain 等实体为节点、以事件为边构成的属性图，存储于 Neo4j |
| **关键路径** | Critical Path | 溯源任务计算出的攻击传播路径，边属性 `analysis.is_path_edge=true` 标记 |
| **顺序流水线** | Sequential Pipeline | 中心机使用单个定时器串行执行轮询、存储、检测、入图四个步骤的架构模式 |

### 1.2 系统组成
系统分为客户机与中心机两部分：

- **客户机侧**：采集数据、转换格式、本地缓冲、提供拉取接口
- **中心机侧**：定时拉取、入库、检测融合、入图、对外 API、异步溯源、报告生成

### 1.3 系统架构图（逻辑视图）
```mermaid
flowchart TB
    subgraph Clients["客户机集群 (≥5 节点)"]
        direction LR
        C1["client-01<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C2["client-02<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C3["client-03<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
        C4["..."]
        C5["client-N<br/>Falco + Filebeat + Suricata<br/>RabbitMQ + API"]
    end

    subgraph Backend["中心机后端 (FastAPI :8001)"]
        Scheduler["定时流水线调度器<br/>单定时器 (5s 周期)"]
        OS_Mod["OpenSearch 模块<br/>入库/检测/融合"]
        Neo4j_Mod["Neo4j 模块<br/>入图/图查询"]
        Analysis_Mod["Analysis 模块<br/>异步溯源任务"]
        API["对外 API<br/>/api/v1/*"]
    end

    subgraph Storage["存储层"]
        OS["OpenSearch :9200<br/>├─ Telemetry 索引<br/>├─ Findings 索引<br/>└─ Task 索引"]
        Neo4j["Neo4j :7687<br/>├─ Entity Graph<br/>├─ 图查询/最短路<br/>└─ 边属性 (溯源结果)"]
    end

    subgraph Frontend["中心机前端 (Next.js :3000)"]
        UI["图谱可视化 UI<br/>├─ 事件/告警查询<br/>├─ 图谱展示<br/>├─ 溯源任务<br/>└─ 报告导出"]
    end

    Clients ==>|"定时拉取<br/>GET /falco,/filebeat,/suricata"| Scheduler
    Scheduler --> OS_Mod
    Scheduler --> Neo4j_Mod
    Scheduler --> Analysis_Mod

    OS_Mod -.->|"写入/读取"| OS
    Neo4j_Mod -.->|"写入/查询"| Neo4j
    Analysis_Mod -.->|"任务状态"| OS
    Analysis_Mod -.->|"结果写回"| Neo4j

    API <--> Scheduler
    API <--> Neo4j_Mod
    API <--> Analysis_Mod

    UI <-->|"HTTP 请求"| API
    UI <-->|"WebSocket"| API

    classDef clientStyle fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef backendStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#bf360c
    classDef storageStyle fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#4a148c
    classDef frontendStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#1b5e20

    class C1,C2,C3,C4,C5 clientStyle
    class Scheduler,OS_Mod,Neo4j_Mod,Analysis_Mod,API backendStyle
    class OS,Neo4j storageStyle
    class UI frontendStyle
```

## 2. 核心数据分层（中心机视角）
### 2.1 数据分层架构
系统数据分层结构如下（自底向上）：

```mermaid
flowchart LR
    subgraph L1["Layer 1: Telemetry 事实层"]
        T1["ECS 事件<br/>event.kind 为 event"]
        T2["OpenSearch<br/>ecs-events-*"]
    end

    subgraph L2["Layer 2: Raw Findings 原始告警层"]
        R1["传感器告警<br/>Security Analytics<br/>event.kind 为 alert<br/>dataset 非 canonical"]
        R2["OpenSearch<br/>raw-findings-*"]
    end

    subgraph L3["Layer 3: Canonical Findings 规范告警层"]
        C1["融合去重<br/>event.kind 为 alert<br/>dataset 为 finding.canonical"]
        C2["OpenSearch<br/>canonical-findings-*"]
    end

    subgraph L4["Layer 4: Entity Graph 图谱层"]
        G1["节点/边<br/>Host/User/Process<br/>File/IP/Domain"]
        G2["Neo4j<br/>Entity Graph"]
    end

    subgraph L5["Layer 5: Trace Task 溯源任务层"]
        A1["任务状态 + 结果<br/>task_id<br/>analysis.* 字段"]
        A2["OpenSearch<br/>analysis-tasks-*"]
        A3["Neo4j<br/>边属性写回"]
    end

    T1 --> T2
    R1 --> R2
    C1 --> C2
    G1 --> G2
    A1 -.->|"状态"| A2
    A1 -.->|"结果"| A3

    T2 -.->|"检测"| R1
    R2 -.->|"融合"| C1
    T2 & C1 -.->|"ECS→Graph"| G1
    G2 -.->|"读取"| A1
    C2 -.->|"输入"| A1

    classDef layerStyle fill:#f5f5f5,stroke:#424242,stroke-width:2px,color:#212121
    classDef storageStyle fill:#e1bee7,stroke:#6a1b9a,stroke-width:2px,color:#4a148c
    classDef dataStyle fill:#bbdefb,stroke:#1565c0,stroke-width:2px,color:#0d47a1

    class L1,L2,L3,L4,L5 layerStyle
    class T2,R2,C2,A2 storageStyle
    class G2,A3 storageStyle
    class T1,R1,C1,G1,A1 dataStyle
```

### 2.2 数据层次说明
| 层次 | 名称 | 判定条件 | 存储位置 | 用途 |
|------|------|----------|----------|------|
| **Layer 1** | Telemetry（事实层） | `event.kind="event"` | OpenSearch `ecs-events-*` | 原始事件检索、检测输入 |
| **Layer 2** | Raw Findings（原始告警层） | `event.kind="alert"` 且 `dataset!="canonical"` | OpenSearch `raw-findings-*` | 原始告警审计、融合输入 |
| **Layer 3** | Canonical Findings（规范告警层） | `event.kind="alert"` 且 `dataset="canonical"` | OpenSearch `canonical-findings-*` | 图谱与溯源主输入 |
| **Layer 4** | Entity Graph（图谱层） | Telemetry + Canonical 转换 | Neo4j 节点/边 | 图查询、路径分析 |
| **Layer 5** | Trace Task（溯源任务层） | 异步任务创建 | OpenSearch `analysis-tasks-*` + Neo4j 边属性 | 任务状态、溯源结果 |

> **说明**：攻击链由图谱边序列与任务结果共同承载，前端展示时以图谱边序列 + 边属性为主，无需单独的链路索引。

## 3. 中心机定时流水线（单定时器顺序流水线）
### 3.1 调度原则
- 中心机使用**单一**定时器触发流水线
- 定时器周期由环境变量 `CENTER_POLL_INTERVAL_SECONDS` 控制，默认 5 秒
- 任一时刻仅运行一个流水线实例：上次流水线未结束时，新触发自动跳过，避免并发写入导致数据漂移与重复

### 3.1.1 设计决策：为什么采用单定时器顺序流水线架构
**决策内容**：采用单个定时器串行执行轮询、存储、检测、入图四个步骤，而非并发或事件驱动架构。

**决策依据**：

1. **数据一致性保证**：串行执行避免了并发写入导致的事件顺序错乱，确保 Telemetry → Raw → Canonical → Graph 的严格因果顺序
2. **资源可控性**：单线程模型使得 CPU、内存、网络带宽消耗可预测，避免高并发场景下的资源竞争
3. **故障排查简化**：串行流水线的调用链清晰，便于通过日志追踪问题根因
4. **实现简洁性**：避免引入分布式锁、消息队列去重等复杂机制

**代价与权衡**：

- 吞吐量上限受限于单线程处理速度，预计可支撑 ~10 客户机规模
- 单步骤故障会导致后续步骤阻塞，需通过快速失败与错误隔离缓解

### 3.3 流水线输入与输出
- **输入**：所有已注册客户机的新数据（轮询拉取接口返回，格式为 ECS 文档）
- **输出**：
  - OpenSearch：Telemetry / Raw Findings / Canonical Findings / Tasks
  - Neo4j：Entity Graph（含溯源任务写回的边属性）

### 3.4 四步顺序与职责（每次 tick 内严格顺序执行）
#### Step 1：从客户机拉取数据
对每个已注册客户机依次执行：

1. 读取注册表条目（含 `listen_url`、`client_token_hash` 等元数据）
2. 调用客户机拉取接口（详见 `../80-规范/87-客户机与中心机接口.md`）
3. 客户机从 RabbitMQ 队列取出消息并返回 ECS 文档列表：
   - 队列为空则返回空数组
   - 拉取操作消费队列消息（队列语义保证增量）

#### Step 2：写入 OpenSearch（字段处理）
中心机对每条 ECS 文档执行字段处理（细节见 `../50-详细设计/中心机/62-OpenSearch存储与索引治理.md` 与 `../80-规范/81-ECS字段规范.md`）：

- 三时间字段补齐与覆盖规则
- `event.kind` 与 `event.dataset` 校验
- `event.id` 补齐与幂等去重
- 按 `event.kind/event.dataset` 路由写入对应索引

#### Step 3：Store-first 检测 + Raw→Canonical 融合
每个 tick 内执行：

1. 触发 Store-first 检测：OpenSearch Security Analytics 扫描 Telemetry 索引
2. 读取检测结果并转换为 ECS Finding 文档，写入 `raw-findings-*`
3. 对指定时间窗内的 Raw Findings 融合去重，生成 Canonical Findings，写入 `canonical-findings-*`

融合去重的指纹规则、provider 合并规则与幂等规则由 `../50-详细设计/中心机/62-OpenSearch存储与索引治理.md` 定义。

> 实现口径（与当前代码对齐）：Step 3 在每个 tick 内固定执行，持续产出 Raw→Canonical 的规范告警层

#### Step 4：触发 ECS→Graph 写入 Neo4j
每个 tick 内执行入图：

1. 以 Canonical Findings 为主，补充必要的 Telemetry（用于补边与证据回溯）
2. 对每条输入文档执行 ECS→Graph 转换
3. 将节点/边写入 Neo4j，满足 `../80-规范/84-Neo4j实体图谱规范.md` 的唯一键与边属性规范
4. 边必须写入 `ts_float`（数值时间戳），支撑时间窗查询与图算法投影

> **实现口径**：Step 4 在每个 tick 内固定执行；入图以 `event.ingested`（中心机入库时间）为窗口边界，确保本 tick 生成的数据不遗漏

### 3.5 流水线时序图
```mermaid
sequenceDiagram
    autonumber
    participant Timer as 定时器<br/>(5s)
    participant Poller as 轮询服务
    participant Clients as 客户机集群
    participant OS as OpenSearch
    participant SA as Security<br/>Analytics
    participant Neo4j as Neo4j

    Note over Timer,Neo4j: 单次 tick 流水线（严格顺序执行）

    Timer->>Poller: 触发 tick
    activate Poller

    rect rgb(240, 248, 255)
        Note right of Poller: Step 1: 拉取数据
        Poller->>OS: 读取 client-registry
        OS-->>Poller: 返回客户机列表

        loop 每个已注册客户机
            Poller->>Clients: GET /falco,/filebeat,/suricata
            Clients-->>Poller: ECS 事件列表
        end
    end

    rect rgb(255, 243, 224)
        Note right of Poller: Step 2: 写入 OpenSearch
        Poller->>OS: store_events(批量事件)
        Note right of OS: 字段处理<br/>- 三时间字段<br/>- event.id 去重<br/>- 索引路由
        OS-->>Poller: 写入完成
    end

    rect rgb(243, 255, 240)
        Note right of Poller: Step 3: 检测与融合
        Poller->>SA: 触发 Store-first 检测
        SA->>OS: 扫描 ecs-events-*
        SA-->>Poller: 生成 Raw Findings

        Poller->>OS: 融合去重 (Raw→Canonical)
        Note right of OS: 指纹规则<br/>- threat.technique.id<br/>- host.id<br/>- entity_id<br/>- time_bucket
        OS-->>Poller: 写入 Canonical Findings
    end

    rect rgb(255, 240, 245)
        Note right of Poller: Step 4: 入图 Neo4j
        Poller->>OS: 查询本 tick 新数据
        OS-->>Poller: Canonical + Telemetry

        Poller->>Neo4j: 批量写入节点/边
        Note right of Neo4j: ECS→Graph 转换<br/>- MERGE 节点<br/>- MERGE 边<br/>- ts_float 属性
        Neo4j-->>Poller: 入图完成
    end

    deactivate Poller
    Note over Timer: 等待下次触发 (5s)
```

## 4. 前端可视化查询链路
### 4.1 查询边界
- Telemetry / Findings 检索：OpenSearch
- 图谱可视化（节点/边）与路径查询：Neo4j
- 前端仅通过后端 API 访问数据，禁止直连数据库

### 4.2 图可视化最小闭环
1. 前端请求后端图查询接口（时间窗边查询、告警边查询、时间窗最短路）
2. 后端调用 Neo4j 模块，返回图数据（nodes/edges）
3. 前端渲染图，用户可点击节点查看属性与相关边的证据引用

后端路由定义（已实现）：`/api/v1/graph/query`，模块内部规则见 `../50-详细设计/中心机/64-Neo4j入图与图查询.md`

## 5. 异步溯源任务
### 5.1 为什么必须异步
节点溯源涉及以下耗时操作：

- 子图扩展与多跳查询
- 图算法（最短路、风险权重路径等）
- 解释性生成（TTP 解释与文本说明）

上述步骤耗时不稳定，必须以异步任务执行，并向前端提供可轮询的进度状态

### 5.2 任务模型（状态机）
每个溯源任务在 OpenSearch `analysis-tasks-*` 保存一条任务文档，任务状态只能取以下值：

```mermaid
stateDiagram-v2
    [*] --> queued: 创建任务

    queued --> running: 任务执行器取走

    running --> succeeded: 执行成功
    running --> failed: 执行失败

    succeeded --> [*]
    failed --> [*]

    note right of queued
        task.progress = 0
        task.created_at
    end note

    note right of running
        task.progress = 5-95
        任务执行中
    end note

    note right of succeeded
        task.progress = 100
        task.finished_at
        结果写回 Neo4j
    end note

    note right of failed
        task.progress < 100
        task.finished_at
        task.error
    end note
```

**状态说明**：

- `queued`：已创建，等待执行
- `running`：执行中
- `succeeded`：已完成
- `failed`：失败（含错误信息）

**状态转移约束**：
- 只允许 `queued → running → succeeded/failed` 单向转移
- 禁止回退与跳转（如 `running → queued`、`queued → succeeded`）

### 5.3 创建、执行与结果写回
1. 前端在图上选定目标节点后，请求后端创建任务
2. 后端立即返回 `task_id`
3. Analysis 模块异步执行：从 Neo4j 读取数据、执行算法、生成关键边集合 + 风险评分 + Technique/Tactic 摘要 + 解释文本
4. Analysis 模块将结果写回 Neo4j 边属性，形成被标注的图
5. 前端轮询任务状态，任务完成后再次请求图查询接口，读取边属性并展示溯源结果

**后端 API**：

- 创建任务：`POST /api/v1/analysis/tasks`
- 查询任务状态：`GET /api/v1/analysis/tasks/{task_id}`
- 拉取写回边：`POST /api/v1/graph/query`（`analysis_edges_by_task` 或 `edges_in_window` + 前端过滤）

写回字段命名、覆盖规则与数据类型由 `../50-详细设计/分析/70-任务模型与状态机.md` 与 `../50-详细设计/中心机/64-Neo4j入图与图查询.md` 定义

### 5.4 KillChain 分析（内部实现）
Analysis 模块使用 KillChain 算法重建攻击路径。

算法基于 MITRE ATT&CK 战术分段，通过有限状态自动机（FSA）识别攻击阶段，并使用大语言模型（LLM）选择最合理的段间连接路径，生成完整攻击链解释。

**核心能力**：

1. **战术分段**：基于 MITRE ATT&CK 战术自动分段（Initial Access → Execution → Privilege Escalation → Lateral Movement → C2 → Impact）
2. **路径重建**：识别各战术阶段之间的连接路径
3. **智能选择**：使用 LLM 从候选路径中选择最合理的攻击链
4. **可解释性**：生成 10-20 句中文的全链解释，包含主谓宾结构
5. **置信度评估**：输出可信度评分（0.0-1.0）

**输出结果**：

- `segments[]`：战术分段列表（含状态、时间范围、锚点、异常边摘要）
- `selected_paths[]`：段间连接路径列表
- `explanation`：LLM 生成的完整攻击链解释
- `confidence`：可信度评分

**持久化方式**：

- Neo4j 边属性：`custom.killchain.uuid`（ECS 合规字段）
- 任务文档：`task.result.killchain_uuid` 和 `task.result.killchain`

详细设计见：`50-详细设计/分析/69-KillChain概览设计.md` 和 `74-KillChain结果展示规范.md`

## 6. 安全与审计边界（课程演示）
- **运行环境**：靶场内网。中心机所有服务与数据库端口仅对靶场内可达网络开放
- **CTI 数据**：使用离线 ATT&CK Enterprise CTI 数据包，运行时不依赖外网
- **数据留痕**：所有结论必须能回溯到 `event.id`，并可定位到数据源类型与时间窗范围（证据链）


# 二、数据流与时序


系统包含五类数据对象：

1. **Telemetry**：事实事件（ECS，`event.kind="event"`），存储于 OpenSearch `ecs-events-*`
2. **Raw Finding**：原始告警（ECS，`event.kind="alert"` 且非 canonical），存储于 OpenSearch `raw-findings-*`
3. **Canonical Finding**：规范告警（ECS，`event.kind="alert"` 且 `event.dataset="finding.canonical"`），存储于 OpenSearch `canonical-findings-*`
4. **Entity Graph**：实体关系图（Neo4j），输入为 Telemetry + Canonical Finding
5. **Trace Task**：溯源任务（OpenSearch 任务索引 + Neo4j 边属性写回）

## 2. 端到端数据流（中心机单定时器流水线）
### 2.1 客户机侧（每台主机）
1. 传感器采集并输出：Filebeat / Falco / Suricata
2. 客户机将采集结果转换为 ECS 文档（字段口径见 `../80-规范/81-ECS字段规范.md`）
3. ECS 文档写入本机 RabbitMQ 队列作为缓冲
4. 客户机对外提供拉取接口（从队列取出并返回），供中心机轮询（接口口径见 `../80-规范/87-客户机与中心机接口.md`）

### 2.2 中心机侧（每次 tick，严格顺序）
中心机定时器每次触发按顺序执行：

1. **拉取**：从所有已注册客户机拉取新数据
2. **入库**：写入 OpenSearch（Telemetry/Raw Findings 路由，三时间字段处理与幂等去重）
3. **检测与融合**：Store-first 检测产出 Raw Findings，并融合生成 Canonical Findings 写回 OpenSearch
4. **入图**：以 Canonical Findings 为主、补充必要 Telemetry，转换为图并写入 Neo4j

上述四步由同一个定时器驱动，形成顺序流水线，系统规格见 `40-概要设计报告.md`

> **实现口径**：
>
> - 后端每个 tick 固定执行 Step 1/2/3/4，形成单定时器顺序流水线
> - Step 4 入图以本 tick 的 `event.ingested` 时间窗为边界，确保本 tick 生成的 Canonical 不会因 `@timestamp` 较早而遗漏

## 3. 前端可视化数据流
1. 用户打开前端页面
2. 前端请求后端：
   - 查事件/告警：后端查询 OpenSearch
   - 查图：后端查询 Neo4j
3. 前端渲染：
   - 事件时间线（来自 OpenSearch）
   - 图谱（来自 Neo4j 返回的 nodes/edges）

## 4. 溯源任务数据流
1. 用户在图上选定一个节点（node uid）
2. 前端请求后端创建溯源任务，后端立即返回 `task_id`
3. Analysis 模块异步执行：读取 Neo4j 子图 → 算法分析 → 生成关键路径与解释
4. Analysis 模块将结果写回 Neo4j 边属性（字段口径见 `32/33`）
5. 前端轮询任务状态，任务完成后再次请求图查询接口，读取边属性并展示溯源结果
6. 前端导出报告：包含告警、图谱与溯源结果


# 三、部署拓扑与网络规划


本文件定义本项目靶场的节点拓扑、网络规划、端口规划与访问边界，保证现场演示的部署一致性与可复现性。

## 读者对象
- 靶场负责人
- 负责部署与编排的同学
- 负责答辩演示的同学

## 引用关系
- 详细部署步骤：`../90-运维与靶场/91-靶场部署.md`
- 一键编排：`../90-运维与靶场/92-一键编排.md`
- 接口与端口约定：`../80-规范/87-客户机与中心机接口.md`

## 1. 节点与角色
靶场节点角色固定为：

- `center`：中心机（OpenSearch、Neo4j、后端 API、前端）
- `client-01` 到 `client-04`：客户机节点（采集与缓冲）
- `c2`：命令与控制服务器（用于产生可观测通信证据）

## 2. 网络规划
网络规划遵循以下原则：

1. 靶场网络与宿主机外网隔离，所有演示数据在靶场内闭环
2. 中心机对外暴露的端口只面向靶场网络可达范围
3. 客户机只向中心机暴露拉取接口，不对外提供其他服务端口

## 3. 端口规划
端口清单与用途如下：

| 组件 | 端口 | 协议 | 访问控制 |
|---|---:|---|---|
| center 后端 | 8001 | HTTP | 仅内网 |
| center 前端 | 3000 | HTTP | 公网（可选） |
| OpenSearch | 9200 | HTTP | 仅本地 |
| Neo4j Bolt | 7687 | Bolt | 仅本地 |
| Neo4j Browser | 7474 | HTTP | 仅本地 |
| 客户机拉取 API | 8888 | HTTP | 仅内网 |

## 4. 部署拓扑
部署拓扑遵循中心机集中、客户机分布的结构：

1. 客户机采集数据并缓冲在本地队列
2. 中心机周期性轮询拉取客户机数据
3. 中心机写入 OpenSearch，并根据需要触发检测与告警融合
4. 中心机将数据入图到 Neo4j，前端通过中心机 API 查询展示

## 5. 安全边界与隔离
1. 中心机不运行未知样本，不执行来自外部的不可信代码
2. 靶场网络中所有组件的访问鉴权与令牌由接口规范定义
3. 现场演示以稳定性为第一目标，任何需要人工介入的步骤都必须写入运维文档


# 四、非功能设计


本文件将需求中的非功能要求落地为确定的工程设计约束，覆盖幂等、一致性、可解释、性能、稳定性与可复现性。

## 读者对象
- 负责系统架构与后端实现的同学
- 负责测试与验收的同学

## 引用关系
- 需求分析：`../20-需求与验收/20-需求分析报告.md`
- 数据与规范：`../80-规范/`
- 测试报告：`../96-测试/96-测试分析报告.md`

## 1. 幂等与去重
### 1.1 OpenSearch 幂等
系统以 `event.id` 作为全局幂等键：

- 同一 `event.id` 重复写入 OpenSearch 不产生重复文档
- `event.id` 的生成规则由 `../80-规范/81-ECS字段规范.md` 定义

### 1.2 图谱幂等
Neo4j 节点写入使用唯一键约束保证幂等；边写入携带证据引用并具备可过滤的去重口径，具体规范见：

- `../80-规范/84-Neo4j实体图谱规范.md`
- `../80-规范/85-溯源结果写回规范.md`

## 2. 一致性与可回放
### 2.1 数据规模假设
系统设计与性能目标基于以下数据规模假设

| 指标 | 规模 | 说明 |
|------|------|------|
| 单客户机每分钟事件量 | ~1000 条 | 三传感器（Falco/Filebeat/Suricata）合计 |
| 单客户机每天事件量 | ~144 万条 | 按 24 小时连续运行计算 |
| 10 客户机集群每天事件量 | ~1440 万条 | 典型部署规模 |
| 图谱节点规模（30天窗口） | ~10 万节点 | Host/User/Process/File/IP/Domain 等实体 |
| 图谱边规模（30天窗口） | ~50 万边 | 包含时间属性的关系边 |
| Raw Findings 每天生成量 | ~1000 条 | 基于 Sigma 与 Security Analytics 检测 |
| Canonical Findings 每天生成量 | ~100 条 | 经融合去重后的规范告警 |
| 单次溯源任务查询时间窗 | 1-60 分钟 | 用户可选的时间窗范围 |

### 2.2 一致性目标
一致性目标：

1. 同一批输入事件在重复拉取与重复执行下，OpenSearch 与 Neo4j 的关键输出一致
2. 任意告警与溯源结论可回溯到 Telemetry 的 `event.id` 证据

## 3. 可解释与证据链
可解释性要求：

- 前端展示的告警、边与溯源结果必须包含证据引用
- 溯源写回字段使用统一前缀 `analysis.`，字段集合与覆盖规则由 `../80-规范/85-溯源结果写回规范.md` 定义

## 4. 性能目标与容量边界
性能目标：

- 图查询在秒级返回满足可视化展示的数据量
- 溯源任务允许长耗时，但必须提供可轮询的状态与进度

容量边界与数据保留策略见：

- `../80-规范/80-数据对象与生命周期.md`

### 4.1 前端查询性能
| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 图查询响应时间 | <3 秒 | 实际负载测试 |
| 事件检索响应时间 | <1 秒 | 实际负载测试 |
| 页面首次渲染 | <2 秒 | 浏览器性能测试 |

### 4.2 中心机轮询性能
| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 单次轮询耗时 | <10 秒 | 轮询日志统计 |
| 单轮拉取吞吐量 | >1000 evt/s | 轮询日志统计 |
| 数据积压恢复时间 | <5 分钟 | 积压场景测试 |

### 4.3 Neo4j 入图性能
| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 100 事件入图耗时 | <1 秒 | 性能测试 `test_batch_ingest_performance` |
| 1000 事件入图耗时 | <5 秒 | 性能测试 `test_batch_ingest_performance` |
| 单事件平均网络往返 | <2 次 | 实现：批量 UNWIND MERGE |
| 批量写入吞吐量 | >100 evt/s | 实际负载测试 |

## 5. 稳定性与故障处理
### 5.1 故障处理原则
故障处理原则：

1. 单次轮询失败不影响后续轮询继续执行
2. LLM 调用失败必须走固定回退路径，保证演示不因外部服务中断而失败
3. 数据重置与复现流程固定，见 `../90-运维与靶场/95-重置复现与排障.md`

### 5.2 失败场景与处理策略
| 失败场景 | 检测方式 | 恢复策略 | 用户影响 |
|---|---|---|---|
| 客户机进程崩溃 | 中心机轮询超时 | 客户机自动重启（Docker restart policy） | 数据缺失，可从历史日志恢复 |
| RabbitMQ 消息队列满 | 队列写入失败 | 告警 + 手动扩容 | 实时性下降，需人工介入 |
| OpenSearch 写入失败 | 写入错误响应 | 重试 3 次 → 失败则丢弃 + 告警 | 事件丢失 |
| Neo4j 连接断开 | Cypher 查询异常 | 自动重连 + 重试 | 图谱更新延迟 |
| 前端 API 调用超时 | HTTP timeout | 前端重试 + 错误提示 | 页面功能暂时不可用 |
| LLM 服务不可用 | API 调用失败 | 走固定回退路径（基于规则的解释生成） | 解释文本质量下降，功能仍可用 |

