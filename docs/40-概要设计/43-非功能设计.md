# 非功能设计

## 文档目的

本文档将需求分析中的非功能要求转化为可执行、可验证的工程设计约束，涵盖幂等性、一致性、可解释性、性能、稳定性与可复现性六个维度。这些约束为系统开发提供明确的验收标准，确保交付的系统不仅功能完整，而且质量可靠、易于维护。

## 读者对象

本文档主要面向两类读者：系统架构师与后端开发人员依据这些约束设计方案；测试与验收人员将其作为验证清单，逐项确认系统是否达到预期标准。

## 引用关系

非功能设计源于需求，体现于规范，验证于测试。需求分析中的非功能要求由 `../20-需求与验收/20-需求分析报告.md` 提出；具体的数据口径与技术规范在 `../80-规范/` 目录中详细定义；最终的验证结果则记录于 `../96-测试/96-测试分析报告.md`。

## 1. 幂等与去重

### 1.1 OpenSearch 幂等

OpenSearch 层以 `event.id` 作为全局唯一键实现幂等性。同一 `event.id` 重复写入不会产生重复文档，即使客户机数据因网络波动被多次拉取，中心机入库后仍保持数据一致性。`event.id` 的生成规则由 `../80-规范/81-ECS字段规范.md` 定义，确保跨客户机、跨时间的全局唯一性。

### 1.2 图谱幂等

Neo4j 的节点写入通过唯一键约束（Unique Constraint）保证幂等性，同一实体的重复写入只会更新属性而不会创建重复节点。边的写入相对复杂：图谱中的"同一条边"可能对应多个不同时间戳的事件（如同一用户在不同时间访问同一文件），边写入携带完整的证据引用（`event_ids` 列表），并提供可过滤的去重口径，允许前端根据时间窗、事件类型等维度灵活去重。具体规范详见 `../80-规范/84-Neo4j实体图谱规范.md` 与 `../80-规范/85-溯源结果写回规范.md`。

## 2. 一致性与可回放

### 2.1 数据规模假设

系统设计与性能目标建立在明确的数据规模假设之上，既为容量规划提供依据，也为性能测试设定基准。

单个客户机在正常运行状态下每分钟产生约 1000 条事件，这是 Falco（系统调用）、Filebeat（系统日志）、Suricata（网络流量）三类传感器的合计输出。按 24 小时连续运行计算，单客户机每天累积约 144 万条事件。10 客户机集群的典型部署规模下，每天事件总量约 1440 万条。图谱规模假设基于 30 天时间窗：预计节点数约 10 万（涵盖 Host、User、Process、File、IP、Domain 等各类实体），边数约 50 万（包含时间属性的关系边）。检测层面，Raw Findings 每天生成约 1000 条（基于 Sigma 规则与 Security Analytics 检测器），经融合去重后的 Canonical Findings 每天约 100 条。溯源任务的时间窗范围为 1-60 分钟，用户可根据分析需求灵活选择。

| 指标 | 规模 | 说明 |
|------|------|------|
| 单客户机每分钟事件量 | ~1000 条 | 三传感器（Falco/Filebeat/Suricata）合计 |
| 单客户机每天事件量 | ~144 万条 | 按 24 小时连续运行计算 |
| 10 客户机集群每天事件量 | ~1440 万条 | 典型部署规模 |
| 图谱节点规模（30天窗口） | ~10 万节点 | Host/User/Process/File/IP/Domain 等实体 |
| 图谱边规模（30天窗口） | ~50 万边 | 包含时间属性的关系边 |
| Raw Findings 每天生成量 | ~1000 条 | 基于 Sigma 与 Security Analytics 检测 |
| Canonical Findings 每天生成量 | ~100 条 | 经融合去重后的规范告警 |
| 单次溯源任务查询时间窗 | 1-60 分钟 | 用户可选的时间窗范围 |

### 2.2 一致性目标

一致性是系统可信度的基石，体现在两个层面：输出一致性要求同一批输入事件在重复拉取与重复执行场景下，OpenSearch 与 Neo4j 的关键输出必须保持一致，不会因重试、重启等操作产生偏差；可回溯性要求任意告警与溯源结论都必须能够追溯到原始遥测数据的 `event.id`，确保每个结论都有据可查、有迹可循。

## 3. 可解释与证据链

可解释性是安全分析系统的核心价值。前端展示的所有告警、图谱边与溯源结果都必须包含完整的证据引用，用户点击任意结论都能追溯到支撑该结论的原始事件。溯源任务写回 Neo4j 的边属性统一使用 `analysis.` 前缀，字段集合、数据类型与覆盖规则由 `../80-规范/85-溯源结果写回规范.md` 定义，确保溯源结果的结构化存储与标准化访问。

## 4. 性能目标与容量边界

性能目标围绕"响应及时、不阻塞、可预测"三个原则设定。图查询要求在秒级内返回足够可视化展示的数据量，确保用户交互流畅。溯源任务允许长耗时（图算法与 LLM 调用本身耗时较大），但必须提供可轮询的进度状态接口，让用户实时了解任务进展。容量边界与数据保留策略由 `../80-规范/80-数据对象与生命周期.md` 详细阐述，涵盖索引生命周期管理、数据轮转策略等运维设计。

### 4.1 前端查询性能

| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 图查询响应时间 | <3 秒 | 实际负载测试 |
| 事件检索响应时间 | <1 秒 | 实际负载测试 |
| 页面首次渲染 | <2 秒 | 浏览器性能测试 |

### 4.2 中心机轮询性能

| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 单次轮询耗时 | <10 秒 | 轮询日志统计 |
| 单轮拉取吞吐量 | >1000 evt/s | 轮询日志统计 |
| 数据积压恢复时间 | <5 分钟 | 积压场景测试 |

### 4.3 Neo4j 入图性能

| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| 100 事件入图耗时 | <1 秒 | 性能测试 `test_batch_ingest_performance` |
| 1000 事件入图耗时 | <5 秒 | 性能测试 `test_batch_ingest_performance` |
| 单事件平均网络往返 | <2 次 | 实现：批量 UNWIND MERGE |
| 批量写入吞吐量 | >100 evt/s | 实际负载测试 |

## 5. 稳定性与故障处理

### 5.1 故障处理原则

系统稳定性建立在"故障隔离、快速恢复、降级可用"三大原则之上。单次轮询失败不应影响后续轮询继续执行，避免局部故障演变为全局瘫痪。LLM 调用失败必须走固定的回退路径（如基于规则生成简化解释），确保演示不因外部服务中断而完全失败。数据重置与复现流程必须标准化、文档化（详见 `../90-运维与靶场/95-重置复现与排障.md`），任何异常场景都有明确的操作指南。

### 5.2 失败场景与处理策略

系统针对各类可能的失败场景，制定了明确的检测方式、恢复策略与用户影响评估。

客户机进程崩溃可通过中心机轮询超时检测，依赖 Docker 的 restart policy 实现自动重启，受影响的时间窗内数据可能缺失但可从历史日志恢复。RabbitMQ 消息队列满时队列写入会失败，此时触发告警并需人工介入扩容，实时性下降但系统仍可运行。OpenSearch 写入失败时系统自动重试 3 次，仍失败则丢弃该事件并告警，牺牲小部分数据以保证整体可用性。Neo4j 连接断开通过 Cypher 查询异常检测，自动重连并重试操作，影响仅限于图谱更新短暂延迟。前端 API 调用超时由 HTTP timeout 检测，前端自动重试并展示友好错误提示，页面功能可能暂时不可用但不影响已加载数据。LLM 服务不可用时系统走固定回退路径（基于规则生成简化解释），解释文本质量下降但核心功能仍可用，体现"降级可用"的设计理念。

| 失败场景 | 检测方式 | 恢复策略 | 用户影响 |
|---|---|---|---|
| 客户机进程崩溃 | 中心机轮询超时 | 客户机自动重启（Docker restart policy） | 数据缺失，可从历史日志恢复 |
| RabbitMQ 消息队列满 | 队列写入失败 | 告警 + 手动扩容 | 实时性下降，需人工介入 |
| OpenSearch 写入失败 | 写入错误响应 | 重试 3 次 → 失败则丢弃 + 告警 | 事件丢失 |
| Neo4j 连接断开 | Cypher 查询异常 | 自动重连 + 重试 | 图谱更新延迟 |
| 前端 API 调用超时 | HTTP timeout | 前端重试 + 错误提示 | 页面功能暂时不可用 |
| LLM 服务不可用 | API 调用失败 | 走固定回退路径（基于规则的解释生成） | 解释文本质量下降，功能仍可用 |

